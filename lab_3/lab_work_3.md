# Лабораторная работа №2

## Дано

> **Внимание**: текст на скачивание отличается от текста для лабораторной работы №2. 
> А именно, там убраны последние несколько тысяч строк, представляющие простое перечисление
> слов, не являющееся связным текстом.

1. Заданный английский текст большого размера.
Его нужно
[скачать](https://www.dropbox.com/s/0ww83i3tsupxfhz/not_so_big_reference_text.txt?dl=1)
и положить в папку `lab_3`. Таким образом, повторить структуру:
```
|-- 2018-2-level-labs
  |-- lab_3
    |-- not_so_big_reference_text.txt
```
2. Произвольный английский текст в виде мнострочной строки.
Произвольность означает наличие любых симоволов, в том числе знаки
препинания и числа. Строка может быть пустой.

Пример произвольного английского текста: 'Mary was quick to realize that
she had won the prize that was a desired thing that everyone wanted'

## Что надо сделать

### Шаг 0.1 Подготовка (проделать вместе с преподавателем на практике).

1. В имеющемся форке репозитория, обновить содержимое до последнего доступного
состояния в родительском репозитории.
2. Изменить файл `main.py`
3. Закоммитить изменения и создать pull request

### Шаг 0.2 Прочитать заданный текст из файла

Уже сделано, обратите внимание на содержимое файла `main.py` ближе к
концу:
```python
if __name__ == '__main__':
  with open('not_so_big_reference_text.txt', 'r') as f:
    REFERENCE_TEXT = f.read()
```

### Шаг 1. Разбиение текста на предложения и токенизация

Заданный английский текст необходимо разбить на предложения. Таким образом, получить
список из строк. В качестве разделителя использовать точку и факт того, что следующее слово
начинается с большой буквы. Все строки (предложения)
в нижнем регистре, не содержат никаких знаков пунктуации, в том числе и замыкающую
точку. 

Например, для текста `Mar#y wa$nted, to swim! However, she was afraid of sharks.` должен
получиться 

```json
[
  [
    'mary', 'wanted', 'to', 'swim'
  ],
  [
    'however', 'she', 'was', 'afraid', 'of', 'sharks'
  ]
]
```

> **Внимание**: предполагается массивное переиспользование логики из первых двух
лабораторных работ.

Внешний интерфейс выглядит так:

```python
def split_by_sentence(text: string) -> list:
  pass
```

### Шаг 2. Создание хранилища соответствий слово-число

Необходимо каждому слову из заданного текста присовить некоторый уникальный идентификатор
(id). Это требуется для того, чтобы работать не со строками напрямую, а с числами,
которые их представляют.

Например, есть слово "experience", поставим ему в соответствие некоторое уникальное 
число - 12345. Следующему слову "elimination" - 12346 и так далее. Выбор правила для присваивания 
идентификатора (счетчик, начинающийся с нуля или с заданного значения) - произвольный - на ваш
выбор. Внешний интефейс такой: это класс `WordStorage`:

```python
class WordStorage:
  pass
```

### Шаг 2.1 Реализация метода наполнения хранилища новым словом

Для добавления слова в хранилище, реализуйте метод `put`, который принимает на вход новое слово
и возвращает его идентификатор.

```python
class WordStorage:
  ...
  def put(self, word:string) -> number:
    pass
```

### Шаг 2.2 Реализация метода получения идентификатора для уже имеющегося слова

Для любого слова можно попытаться получить его `id`. Для этого, реализуйте метод `get_id_of`, который
принимает на вход слово и возвращает его идентификатор. Если слово неизвестное, возвращается 
`None`.

```python
class WordStorage:
  ...
  def get_id_of(self, word:string) -> number:
    pass
```

### Шаг 2.3 Реализация метода получения оригинального слова по заданному идентификатору

Для любого `id` можно попытаться получить соответствующее ему слово. Для этого, реализуйте 
метод `get_original_by(id:number)`, который принимает на вход идентификатор и возвращает слово. 
Если идентификатор неизвестный, возвращается `None`.

```python
class WordStorage:
  ...
  def get_original_by(self, id:number) -> string:
    pass
```

### Шаг 2.4 Заполнение хранилища словами из корпуса (списка) предложений

Для этого воспользуемся полученным в результате Шага №1 списком предложений и
добавим все слова из него в экземпляр класса `WordStorage` с помощью метода
`from_corpus`. Готовый заполненный экземпляр будем активно использовать далее.

```python
class WordStorage:
  ...
  def from_corpus(self, id:tuple) -> string:
    pass
```

### Шаг 3. Кодирование (encoding) корпуса (списка) предложений

Полученный в результате Шага №1 корпус предложений необходимо кодировать с 
помощью заполненного экземпляра класса `WordStorage`. Кодирование
заключается в замене слов на соответствующие им идентификаторы.

Например, корпус из одно предложения:

```json
[
  [
    "experience", "elimination"
  ]
]
```

превращается в:

```json
[
  [
    12345, 12346
  ]
]
```

```python
def encode(storage_instance, corpus) -> list:
  pass
```

### Шаг 4. Создание структуры для хранения Н-грамм (N-gram)

Для успешного предсказания следующего слова на основании заданного контекста,
необходимо построить абстракцию, которая предоставляет достаточный для
этого интерфейс. Создадим класс:

```python
class NGramTrie:
  pass
```

Конструктор должен принимать на вход размер контекста ("N" из названия N-gram - 1 для уни-грамм,
2 - для би-грамм, 3 - для три-грамм и т.д.). Это значения сохранется
в собственном поле экземпляра класса - `size`.

Сами Н-граммы должны храниться в собственном поле экземпляра класса - `gram_frequencies`.
Предлагается использовать для хранения словарь, где ключами выступают кортежи из чисел, а
значения - частота возникновения соответствующего кортежа в тексте.

### Шаг 4.1 Реализация метода заполения Н-грамм из заданного предложения

Класс `NGramTrie` позволяет собрать Н-граммы из заданного предложения.
Например, предложение `Mary wanted to swim`, превращается в:

* несколько би-грамм: `('<s>', 'mary')`, `('mary', 'wanted')`, `('wanted', 'to')`,
  `('to', 'swim')`, `('swim', '</s'>)`.
* несколько три-грамм: `('<s>', 'mary', 'wanted')`, `('mary', 'wanted', 'to')`,
  `('wanted', 'to', 'swim')`, `('to', 'swim', '</s'>)`

> Напомним, что на самом деле на данном этапе вы уже работаете с закодироваными
> предложениями, полученными на Шаге №3. Текстовое описание дано для наглядности.

> Символы`<s>, </s>` обозначают конец и начало предложения. Эти символы должны оказаться частью
> хранилища слов как самостоятельные слова. Попробуйте догадаться сами, зачем это необходимо и
> как это правильно сделать.

> Это не опечатка в названии класса. Такое название выбрано намеренно (`trie` является отраслевым
> термином).

Заполнение происходит с помощью метода:

```python
class NGramTrie:
  ...
  def fill_from_sentence(self, sentence: tuple) -> string:
    pass
```

Возвращает метод код в виде строки: `'OK'` - если все в порядке. `'ERROR'` - если произошла ошибка.

Цель данного метода заполнить внутреннее содержимое класса - `gram_frequencies`,
определенное в Шаге №4.

### Шаг 4.2 Расчет лог-вероятностей для каждой Н-граммы

Для расчета би-грамм воспользуемся следующей формулой: 

<img src="https://latex.codecogs.com/gif.latex?P(w_{n}|w_{n-1})&space;=&space;\frac{C(w_{n-1},w_{n})}{\sum_{w}C(w_{n-1},&space;w)}" title="P(w_{n}|w_{n-1}) = \frac{C(w_{n-1},w_{n})}{\sum_{w}C(w_{n-1}, w)}" />

`C(w_{n}|w_{n-1})` - это количество появлений кортежа `(w_{n-1}, w_{n})` в заданном тексте (частота).
`sum(C(w_{n-1}, w)` - это количество появлений кортежей вида `(w_{n-1} w)` для всех `w` в заданном корпусе.

Идея проста: насколько часто, би-грамма, начинающаяся со слова `w_{n-1}` будет заканчиваться интересующим
нам словом `w_{n}`.

Обощения для N-грамм:

<img src="https://latex.codecogs.com/gif.latex?P(w_{n}|w_{n-1},w_{n-2},...,w_{n-N&plus;1})&space;=&space;\frac{C(w_{n-N&plus;1},&space;...,&space;w_{n-1},&space;w_{n})}{\sum_{w}C(w_{n-N&plus;1},&space;...,&space;w_{n-1},&space;w)}" title="P(w_{n}|w_{n-1},w_{n-2},...,w_{n-N+1}) = \frac{C(w_{n-N+1}, ..., w_{n-1}, w_{n})}{\sum_{w}C(w_{n-N+1}, ..., w_{n-1}, w)}" />

> Внимание, реализация N-грамм требуется для студентов, желающих получить оценку 9 или 10.
> Внимание, для студентов, желающих получить оценку 8 требуется реализовать три-граммы.
> Выполнение би-грамм обязательно для всех студентов.

После получения относительных вероятностей (описаны выше), необходимо взять логарифим по основанию `e`
от полученного отношения. Такие лог-вероятности уже умножать не нужно, вместо этого
воспользуйтесь операцией сложения.

Все значения вероятностей храним в словаре, аналогичном `gram_frequencies`, он должен называться
`gram_log_probabilities`.

### Шаг 5. Генерация текста по заданному слову (набор предложений)

TBD
